## Background

Generalisability is often thought of as the extent that the effect of an intervention in one place predicts what will happen in another place. We have argued in our pre-inception paper that evaluations of complex interventions can best learn from one context what to do in another using ‘mid-level’ theories that consider mechanisms and contingencies for when and how interventions might work. Using theory requires all aspects of an evaluation to be considered together: the impact evaluation, the process evaluation, and relevant literature across a variety of fields. Standards for rigorous evaluation and systematic review should be respected to limit the effects of human biases, especially confirmation bias. Additionally, there should be strategic thought about what to evaluate, where and with whom, so that evaluations contribute maximally to testing and refining theories, as encouraged by Realists evaluators, but without abandoning RCTs.

## Outline

We will build upon the pre-inception paper, which set-out the problems underlying the task of learning the most that we can from evaluations to use the findings for policy. We will hold a consultation within LSHTM to draw on the expertise of the Centre for Evaluation members, and hold a consultation with several ILT members who are part of this team. We will define ‘mid-level’ theory, and discuss how they help evaluations inform policy. During the consultation we will consider what can and should be known about the contexts where evidence is being applied, and review strategies to identify when the structures underlying effects in one place are likely to be important in another. We will conduct a semi-systematic review of approaches to combine process and impact data for inference; and illustrate best-practices with retrospective case-studies. We will explore the extent to which recommended methods are used in practice in development research in focus areas: working in emergency settings and evaluating aid delivery mechanisms. 

We anticipate that the literature will not provide a comprehensive framework for combining process and impact strands for generalizable inference. To address this, we will draw on mixed-methods research, and synthesis methods for reviews for complex interventions. We will also draw on recent work by researchers at the University of East Anglia and elsewhere, who deploy Bayesian models to incorporate a range of data in a transparent way that implicitly updates confidence in the underlying theory. We will refer to literature from causal modelling that has explored transferability of theory using graphical representations.

## Importance

Using what is learned from evaluations to inform decisions in other contexts is a cost-effective and scientific approach to policy making. Better guidance for the conduct of evaluations will improve commissioning of evaluations. Evaluations that lead to better transferability of learning using mid-level theories will build upon each other as they are abstracted from their specific contexts. Learning should also be translatable into action, and this approach to generalisability will emphasise the practicalities of identifying the factors that support or impede an intervention effect. At the first meeting of the ILT, there was considerable convergence of thinking in this area, and enthusiasm for better guidance for using scientific, ‘theory’ based, approaches to evaluation outside of (primarily) academic research. This paper will push-forward the benchmark for collaborative thinking within CEDIL, a key strength of the consortium of academics from different sectors.
